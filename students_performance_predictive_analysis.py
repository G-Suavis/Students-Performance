# -*- coding: utf-8 -*-
"""Students-Performance-predictive analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ivhU9WuYKFQP5-dfZ_k4jX78vUKWi1J3
"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

# Load the dataset
file_path = "/content/StudentPerformanceFactors.csv"  # Path to the CSV file
df = pd.read_csv(file_path)

# Display the first few rows of the dataset
print("Dataset preview:")
print(df.head())

# Preprocessing
# Drop irrelevant columns (customize based on your dataset)
irrelevant_columns = []  # Add any columns that don't contribute to the prediction
df = df.drop(columns=irrelevant_columns, errors='ignore')

# Handle missing values
df = df.fillna(df.median(numeric_only=True))  # Fill numerical columns with median
df = df.fillna("missing")  # Fill categorical columns with "missing"

# Encode categorical variables
categorical_columns = df.select_dtypes(include=['object']).columns
label_encoders = {}
for col in categorical_columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Define features and target
X = df.drop(columns=['Exam_Score'], errors='ignore')  # Features
y = df['Exam_Score']  # Target variable

# Standardize numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Define regression models
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(random_state=42),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "SVR": SVR(kernel='rbf'),
    "XGBoost": XGBRegressor(random_state=42),
    "LightGBM": LGBMRegressor(random_state=42)
}

# Train and evaluate models
results = []
for model_name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)

    # Predict on the test set
    y_pred = model.predict(X_test)

    # Evaluate performance
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    results.append({
        "Model": model_name,
        "MAE": mae,
        "MSE": mse,
        "RMSE": rmse,
        "R2 Score": r2
    })

# Convert results to a DataFrame for better readability
results_df = pd.DataFrame(results).sort_values(by="R2 Score", ascending=False)
print("\nModel Performance Comparison:")
print(results_df)

# Save results to a CSV file
results_df.to_csv("model_performance_comparison.csv", index=False)

# Feature importance for tree-based models
print("\nFeature Importance for Tree-Based Models:")
for model_name in ["Random Forest", "XGBoost", "LightGBM"]:
    model = models[model_name]
    importance = pd.DataFrame({
        "Feature": df.drop(columns=['Exam_Score']).columns,
        "Importance": model.feature_importances_
    }).sort_values(by="Importance", ascending=False)

    print(f"\n{model_name} Feature Importance:")
    print(importance.head(10))

from google.colab import drive
drive.mount('/content/drive')